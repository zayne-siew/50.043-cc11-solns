{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "m9yXqV3LigUA"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32fjpkeS-nYP"
      },
      "source": [
        "# Lab 11 Spark\n",
        "\n",
        "Author: ISTD, SUTD\n",
        "\n",
        "Title: Lab 11, Spark part 1\n",
        "\n",
        "Date: March 5, 2025\n",
        "\n",
        "## Learning outcome\n",
        "\n",
        "\n",
        "By the end of this lesson, you are able to\n",
        "\n",
        "* Submit PySpark jobs to a Spark cluster\n",
        "* Paralelize data processing using PySpark\n",
        "\n",
        "\n",
        "You can either execute this lab directly on the aws cluster with HDFS file system, or you can install PySpark in Google Colab and load the files locally. The main difference in coding is that we do not load the context from the HDFS filesystem, but instead just load a local file. Other than than that, all PySpark commands are the same.\n",
        "\n",
        "To run this lab, you can make a copy of this notebook or `File -> Open in Playground Mode`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9yXqV3LigUA"
      },
      "source": [
        "## Installing PySpark in Google Colab\n",
        "\n",
        "To install PySpark in Google Collab, execute the below cell. This will download Spark and install all necessary libraries for this lab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxv7w_2y2bb9",
        "outputId": "f648d55b-fc2f-4046-b8d3-dea2446723cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "# Check this site for the latest download link https://www.apache.org/dyn/closer.lua/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "!pip install py4j\n",
        "\n",
        "import os\n",
        "import sys\n",
        "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\"\n",
        "\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()\n",
        "\n",
        "import pyspark\n",
        "\n",
        "from pyspark.sql import DataFrame, SparkSession\n",
        "from typing import List\n",
        "import pyspark.sql.types as T\n",
        "import pyspark.sql.functions as F\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:10 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [73.0 kB]\n",
            "Hit:11 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Get:12 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,697 kB]\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,788 kB]\n",
            "Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,847 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,243 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,542 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,101 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [4,161 kB]\n",
            "Fetched 24.7 MB in 11s (2,342 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "47 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "tar: spark-3.2.1-bin-hadoop3.2.tgz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.5)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.11/dist-packages (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wordcount Example\n",
        "\n",
        "Let us first download the necessary data file. We can find it at `https://raw.githubusercontent.com/istd50043-2023-spring/cohort_problems/main/cc11/ex1/data.csv`.\n",
        "\n",
        "Colab lets us execute unix commands, as long as we prepend them with `!`. So let's download the file and move it into a new folder called `input`. While we are at it, let's create a folder called `output` as well."
      ],
      "metadata": {
        "id": "r_DCGGK3-E6s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/sutd50043/cohortclass/main/cc10/data/TheCompleteSherlockHolmes.txt\n",
        "!mkdir input\n",
        "!mv TheCompleteSherlockHolmes.txt input/\n",
        "!mkdir output"
      ],
      "metadata": {
        "id": "z0LtAmEBoSDR",
        "outputId": "61cc8e90-5993-4443-ad15-f4fb486590f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-15 08:07:58--  https://raw.githubusercontent.com/sutd50043/cohortclass/main/cc10/data/TheCompleteSherlockHolmes.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3705628 (3.5M) [text/plain]\n",
            "Saving to: ‘TheCompleteSherlockHolmes.txt’\n",
            "\n",
            "TheCompleteSherlock 100%[===================>]   3.53M  10.9MB/s    in 0.3s    \n",
            "\n",
            "2025-04-15 08:07:59 (10.9 MB/s) - ‘TheCompleteSherlockHolmes.txt’ saved [3705628/3705628]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can check that the data.csv file downloaded by uncollapsing the left panel and checking the folder contents.\n",
        "\n",
        "Now we are ready to write our PySpark code. The goal is to write a simple wordcounter:"
      ],
      "metadata": {
        "id": "7ANacU_Go4bY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "# sc.stop() # uncomment this during debugging to restart your context in case execution stopped mid-way this cell.\n",
        "\n",
        "conf = SparkConf().setAppName(\"Wordcount Application\")\n",
        "sc = SparkContext(conf=conf)\n",
        "spark = SparkSession(sc)\n",
        "\n",
        "# note that we load the text file directly with a local path instead of providing an hdfs url\n",
        "input_file_name = 'input/TheCompleteSherlockHolmes.txt'\n",
        "text_file = sc.textFile(input_file_name)\n",
        "\n",
        "counts = text_file.flatMap(lambda line: line.split(\" \")) \\\n",
        "             .map(lambda word: (word, 1)) \\\n",
        "             .reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "output_folder = './output/word_count'\n",
        "counts.saveAsTextFile(output_folder)\n",
        "\n",
        "sc.stop()"
      ],
      "metadata": {
        "id": "pzse2gw82OFu"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1\n",
        "\n",
        "Write a PySpark application which takes a (set of) Comma-seperated-value (CSV) file(s) with 2 columns and output a CSV file with first two columns same as the input file, and the third column contains the values obtained by splitting the first column using the second column as delimiter.\n",
        "\n",
        "The input file can be found here: `https://raw.githubusercontent.com/sutd50043/cohortclass/main/cc11/ex1/data.csv`.\n",
        "\n",
        "For example, given input from a file:\n",
        "\n",
        "```\n",
        "50000.0#0#0#,#\n",
        "0@1000.0@,@\n",
        "1$,$\n",
        "1000.00^Test_string,^\n",
        "```\n",
        "\n",
        "\n",
        "the program should output\n",
        "\n",
        "```\n",
        "50000.0#0#0#,#,['50000.0', '0', '0']\n",
        "0@1000.0@,@,['0', '1000.0', '']\n",
        "1$,$,['1', '']\n",
        "1000.00^Test_string,^,['1000.00', 'Test_string']\n",
        "```\n",
        "\n",
        "and write it to a file.\n",
        "\n"
      ],
      "metadata": {
        "id": "GZHg9lXG8bwo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/sutd50043/cohortclass/main/cc11/ex1/data.csv\n",
        "!mv data.csv input/data1.csv"
      ],
      "metadata": {
        "id": "_LOVO0mUfw7S",
        "outputId": "c156c881-92a3-46c0-ddf6-ec5e6b56eaf5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-15 08:33:07--  https://raw.githubusercontent.com/sutd50043/cohortclass/main/cc11/ex1/data.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 54 [text/plain]\n",
            "Saving to: ‘data.csv’\n",
            "\n",
            "data.csv            100%[===================>]      54  --.-KB/s    in 0s      \n",
            "\n",
            "2025-04-15 08:33:07 (930 KB/s) - ‘data.csv’ saved [54/54]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "# sc.stop() # uncomment this during debugging to restart your context in case execution stopped mid-way this cell.\n",
        "\n",
        "conf = SparkConf().setAppName(\"Exercise 1\")\n",
        "sc = SparkContext(conf=conf)\n",
        "spark = SparkSession(sc)\n",
        "\n",
        "# note that we load the text file directly with a local path instead of providing an hdfs url\n",
        "input_file_name = 'input/data1.csv'\n",
        "text_file = sc.textFile(input_file_name)\n",
        "\n",
        "def foreach(record: str) -> str:\n",
        "    cols = record.split(',')\n",
        "    if len(cols) > 1:\n",
        "        cols.append(str(cols[0].split(cols[1])))\n",
        "    return ','.join(cols)\n",
        "\n",
        "result = text_file.map(foreach)\n",
        "\n",
        "output_folder = './output/ex1'\n",
        "result.saveAsTextFile(output_folder)\n",
        "\n",
        "sc.stop()"
      ],
      "metadata": {
        "id": "F1aLxWrqRyAk"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2\n",
        "\n",
        "Write PySpark application which aggregates (counts) a (set of) CSV file(s) with 4 columns based on its third column, the destination IP.\n",
        "\n",
        "The input file can be found here: `https://raw.githubusercontent.com/sutd50043/cohortclass/main/cc11/ex2/data.csv`\n",
        "\n",
        "Given input\n",
        "\n",
        "```\n",
        "05:49:56.604899, 10.0.0.2.54880, 10.0.0.3.5001, 2\n",
        "05:49:56.604900, 10.0.0.2.54880, 10.0.0.3.5001, 2\n",
        "05:49:56.604899, 10.0.0.2.54880, 10.0.0.3.5001, 2\n",
        "05:49:56.604900, 10.0.0.2.54880, 10.0.0.3.5001, 2\n",
        "05:49:56.604899, 10.0.0.2.54880, 10.0.0.3.5001, 2\n",
        "05:49:56.604900, 10.0.0.2.54880, 10.0.0.3.5001, 2\n",
        "05:49:56.604899, 10.0.0.2.54880, 10.0.0.3.5001, 2\n",
        "05:49:56.604900, 10.0.0.2.54880, 10.0.0.3.5001, 2\n",
        "05:49:56.604899, 10.0.0.2.54880, 10.0.0.3.5001, 2\n",
        "05:49:56.604900, 10.0.0.2.54880, 10.0.0.3.5001, 2\n",
        "05:49:56.604899, 10.0.0.2.54880, 10.0.0.3.5001, 2\n",
        "05:49:56.604900, 10.0.0.2.54880, 10.0.0.3.5001, 2\n",
        "05:49:56.604899, 10.0.0.2.54880, 10.0.0.3.5001, 2\n",
        "05:49:56.604908, 10.0.0.3.5001, 10.0.0.2.54880, 2\n",
        "05:49:56.604908, 10.0.0.3.5001, 10.0.0.2.54880, 2\n",
        "05:49:56.604908, 10.0.0.3.5001, 10.0.0.2.54880, 2\n",
        "05:49:56.604908, 10.0.0.3.5001, 10.0.0.2.54880, 2\n",
        "05:49:56.604908, 10.0.0.3.5001, 10.0.0.2.54880, 2\n",
        "05:49:56.604908, 10.0.0.3.5001, 10.0.0.2.54880, 2\n",
        "05:49:56.604908, 10.0.0.3.5001, 10.0.0.2.54880, 2\n",
        "```\n",
        "the program should output\n",
        "\n",
        "```\n",
        " 10.0.0.3.5001,13\n",
        " 10.0.0.2.54880,7\n",
        "```"
      ],
      "metadata": {
        "id": "tIYQgNbA963l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/sutd50043/cohortclass/main/cc11/ex2/data.csv\n",
        "!mv data.csv input/data2.csv"
      ],
      "metadata": {
        "id": "pxZk3lV_k3FZ",
        "outputId": "864907f5-fc88-4c70-9ed3-de0868f3d152",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-15 08:32:57--  https://raw.githubusercontent.com/sutd50043/cohortclass/main/cc11/ex2/data.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1000 [text/plain]\n",
            "Saving to: ‘data.csv’\n",
            "\n",
            "\rdata.csv              0%[                    ]       0  --.-KB/s               \rdata.csv            100%[===================>]    1000  --.-KB/s    in 0s      \n",
            "\n",
            "2025-04-15 08:32:57 (18.8 MB/s) - ‘data.csv’ saved [1000/1000]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "# sc.stop() # uncomment this during debugging to restart your context in case execution stopped mid-way this cell.\n",
        "\n",
        "conf = SparkConf().setAppName(\"Exercise 2\")\n",
        "sc = SparkContext(conf=conf)\n",
        "spark = SparkSession(sc)\n",
        "\n",
        "# note that we load the text file directly with a local path instead of providing an hdfs url\n",
        "input_file_name = 'input/data2.csv'\n",
        "text_file = sc.textFile(input_file_name)\n",
        "\n",
        "def foreach(record: str) -> tuple[str, int]:\n",
        "    cols = record.split(', ')\n",
        "    return (cols[2], 1) if len(cols) > 2 else ('', 0)\n",
        "\n",
        "result = text_file.map(foreach).reduceByKey(lambda a, b: a + b).map(lambda row: ','.join(map(str, row)))\n",
        "\n",
        "output_folder = './output/ex2'\n",
        "result.saveAsTextFile(output_folder)\n",
        "\n",
        "sc.stop()"
      ],
      "metadata": {
        "id": "T-coxW5U9690"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 3\n",
        "\n",
        "Given the same input as Exercise 2, write a PySpark application which outputs the following:\n",
        "\n",
        "```\n",
        "05:49:56.604899,10.0.0.2.54880, 10.0.0.3.5001, 2, 13\n",
        "05:49:56.604900,10.0.0.2.54880, 10.0.0.3.5001, 2, 13\n",
        "05:49:56.604899,10.0.0.2.54880, 10.0.0.3.5001, 2, 13\n",
        "05:49:56.604900,10.0.0.2.54880, 10.0.0.3.5001, 2, 13\n",
        "05:49:56.604899,10.0.0.2.54880, 10.0.0.3.5001, 2, 13\n",
        "05:49:56.604900,10.0.0.2.54880, 10.0.0.3.5001, 2, 13\n",
        "05:49:56.604899,10.0.0.2.54880, 10.0.0.3.5001, 2, 13\n",
        "05:49:56.604900,10.0.0.2.54880, 10.0.0.3.5001, 2, 13\n",
        "05:49:56.604899,10.0.0.2.54880, 10.0.0.3.5001, 2, 13\n",
        "05:49:56.604900,10.0.0.2.54880, 10.0.0.3.5001, 2, 13\n",
        "05:49:56.604899,10.0.0.2.54880, 10.0.0.3.5001, 2, 13\n",
        "05:49:56.604900,10.0.0.2.54880, 10.0.0.3.5001, 2, 13\n",
        "05:49:56.604899,10.0.0.2.54880, 10.0.0.3.5001, 2, 13\n",
        "05:49:56.604908, 10.0.0.3.5001,10.0.0.2.54880, 2, 7\n",
        "05:49:56.604908, 10.0.0.3.5001,10.0.0.2.54880, 2, 7\n",
        "05:49:56.604908, 10.0.0.3.5001,10.0.0.2.54880, 2, 7\n",
        "05:49:56.604908, 10.0.0.3.5001,10.0.0.2.54880, 2, 7\n",
        "05:49:56.604908, 10.0.0.3.5001,10.0.0.2.54880, 2, 7\n",
        "05:49:56.604908, 10.0.0.3.5001,10.0.0.2.54880, 2, 7\n",
        "05:49:56.604908, 10.0.0.3.5001,10.0.0.2.54880, 2, 7\n",
        "```\n",
        "\n",
        "\n",
        "In the event the input is very huge with too many unique destination IP values, can your program scale?\n",
        "\n",
        "\n",
        "The questions were adopted from `https://jaceklaskowski.github.io/spark-workshop/exercises/`\n"
      ],
      "metadata": {
        "id": "0BIgiKCh97D0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "# sc.stop() # uncomment this during debugging to restart your context in case execution stopped mid-way this cell.\n",
        "\n",
        "conf = SparkConf().setAppName(\"Exercise 3\")\n",
        "sc = SparkContext(conf=conf)\n",
        "spark = SparkSession(sc)\n",
        "\n",
        "# note that we load the text file directly with a local path instead of providing an hdfs url\n",
        "input_file_name = 'input/data2.csv'\n",
        "text_file = sc.textFile(input_file_name)\n",
        "\n",
        "def foreach(record: str) -> tuple[str, str]:\n",
        "    cols = record.split(', ')\n",
        "    return (cols[2], 1) if len(cols) > 2 else ('', 0)\n",
        "\n",
        "counts = text_file.map(foreach).reduceByKey(lambda a, b: a + b).collectAsMap()\n",
        "\n",
        "def append(record: str) -> str:\n",
        "    cols = record.split(', ')\n",
        "    return record + f', {str(counts[cols[2]]) if len(cols) > 2 else \"1\"}'\n",
        "\n",
        "result = text_file.map(append)\n",
        "\n",
        "output_folder = './output/ex3'\n",
        "result.saveAsTextFile(output_folder)\n",
        "\n",
        "sc.stop()"
      ],
      "metadata": {
        "id": "IkEQswDg97Iu"
      },
      "execution_count": 39,
      "outputs": []
    }
  ]
}